{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514d1d7d",
   "metadata": {},
   "source": [
    "Notebook Created August 2025, Python 3.13.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125737c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read json within keys directory to pick up the keys\n",
    "with open('/Users/vilourenco/Documents/GitHub/azure-ai-engineer/keys/keys.json') as f:\n",
    "    keys = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee15319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "\n",
    "# Initialize the SpeechConfig with the API key and region\n",
    "speech_config = speech_sdk.SpeechConfig(keys[\"azure_foundry\"][\"api_key\"], 'eastus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9e91e",
   "metadata": {},
   "source": [
    "(Text From: https://learn.microsoft.com/en-us/training/modules/create-speech-enabled-apps/)\n",
    "# Speech to Text API\n",
    "\n",
    "The Azure AI Speech service supports speech recognition through the following features:\n",
    "- **Real-time transcription**: Instant transcription with intermediate results for live audio inputs.\n",
    "- **Fast transcription**: Fastest synchronous output for situations with predictable latency.\n",
    "- **Batch transcription**: Efficient processing for large volumes of prerecorded audio.\n",
    "- **Custom speech**: Models with enhanced accuracy for specific domains and conditions.\n",
    "### Using the Azure AI Speech SDK\n",
    "While the specific details vary, depending on the SDK being used (Python, C#, and so on); there's a consistent pattern for using the Speech to text API:\n",
    "\n",
    "![image.png](https://learn.microsoft.com/en-us/training/wwl-data-ai/create-speech-enabled-apps/media/speech-to-text.png)\n",
    "\n",
    " A diagram showing how a SpeechRecognizer object is created from a **SpeechConfig** and **AudioConfig**, and its RecognizeOnceAsync method is used to call the Speech API.\n",
    "1. Use a **SpeechConfig** object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key.\n",
    "2. Optionally, use an **AudioConfig** to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.\n",
    "3. Use the **SpeechConfig** and **AudioConfig** to create a **SpeechRecognizer** object. This object is a proxy client for the Speech to text API.\n",
    "4. Use the methods of the **SpeechRecognizer** object to call the underlying API functions. For example, the RecognizeOnceAsync() method uses the Azure AI Speech service to asynchronously transcribe a single spoken utterance.\n",
    "5. Process the response from the Azure AI Speech service. In the case of the **RecognizeOnceAsync()** method, the result is a **SpeechRecognitionResult** object that includes the following properties:\n",
    "- Duration\n",
    "- OffsetInTicks\n",
    "- Properties\n",
    "- Reason\n",
    "- ResultId\n",
    "- Text\n",
    "If the operation was successful, \n",
    "\n",
    "The Reason property has the enumerated value RecognizedSpeech, and the Text property contains the transcription. Other possible values for Result include NoMatch (indicating that the audio was successfully parsed but no speech was recognized) or Canceled, indicating that an error occurred (in which case, you can check the Properties collection for the CancellationReason property to determine what went wrong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c104ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: Close control.\n"
     ]
    }
   ],
   "source": [
    "# Speech to Text API\n",
    "\n",
    "# Configure\n",
    "speech_config.endpoint_id = keys[\"azure_foundry\"][\"endpoint\"]\n",
    "\n",
    "# Initialize the AudioConfig to use the default microphone\n",
    "audio_config = speech_sdk.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "# Create a SpeechRecognizer object\n",
    "speech_recognizer = speech_sdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# Start speech recognition\n",
    "print(\"Speak into your microphone.\")\n",
    "result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "# Check the result\n",
    "if result.reason == speech_sdk.ResultReason.RecognizedSpeech:\n",
    "    print(f\"Recognized: {result.text}\")\n",
    "elif result.reason == speech_sdk.ResultReason.NoMatch:\n",
    "    print(\"No speech could be recognized.\")\n",
    "elif result.reason == speech_sdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(f\"Speech Recognition canceled: {cancellation_details.reason}\")\n",
    "    if cancellation_details.reason == speech_sdk.CancellationReason.Error:\n",
    "        print(f\"Error details: {cancellation_details.error_details}\")\n",
    "else:\n",
    "    print(f\"Unexpected result: {result.reason}\")\n",
    "# End of code to run speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217fefc1",
   "metadata": {},
   "source": [
    "# Text to Speech API\n",
    "\n",
    "Similarly to its Speech to text APIs, the Azure AI Speech service offers other REST APIs for speech synthesis:\n",
    "\n",
    "- The Text to speech API, which is the primary way to perform speech synthesis.\n",
    "- The Batch synthesis API, which is designed to support batch operations that convert large volumes of text to audio for example to generate an audio-book from the source text.\n",
    "\n",
    "You can learn more about the REST APIs in the <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-synthesis\">Text to speech REST API documentation</a>. In practice, most interactive speech-enabled applications use the Azure AI Speech service through a (programming) language-specific SDK. \n",
    "\n",
    "### Using the Azure AI Speech SDK\n",
    "As with speech recognition, in practice most interactive speech-enabled applications are built using the Azure AI Speech SDK.\n",
    "The pattern for implementing speech synthesis is similar to that of speech recognition:\n",
    "![image.png](https://learn.microsoft.com/en-us/training/wwl-data-ai/create-speech-enabled-apps/media/text-to-speech.png)\n",
    "\n",
    "1. Use a SpeechConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key.\n",
    "2. Optionally, use an AudioConfig to define the output device for the speech to be synthesized. By default, this is the default system speaker, but you can also specify an audio file, or by explicitly setting this value to a null value, you can process the audio stream object that is returned directly.\n",
    "3. Use the SpeechConfig and AudioConfig to create a SpeechSynthesizer object. This object is a proxy client for the Text to speech API.\n",
    "4. Use the methods of the SpeechSynthesizer object to call the underlying API functions. For example, the SpeakTextAsync() method uses the Azure AI Speech service to convert text to spoken audio.\n",
    "5. Process the response from the Azure AI Speech service. In the case of the SpeakTextAsync method, the result is a SpeechSynthesisResult object that contains the following properties:\n",
    "- AudioData\n",
    "- Properties\n",
    "- Reason\n",
    "- ResultId\n",
    "\n",
    "When speech has been successfully synthesized, the Reason property is set to the SynthesizingAudioCompleted enumeration and the AudioData property contains the audio stream (which, depending on the AudioConfig may have been automatically sent to a speaker or file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "731ed9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech synthesis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Text to Speech API\n",
    "# Initialize the SpeechConfig with the API key and region\n",
    "speech_config = speech_sdk.SpeechConfig(subscription=keys[\"azure_foundry\"][\"api_key\"], region='eastus')\n",
    "\n",
    "\"\"\" Set the speech synthesis output format\n",
    "    This example uses Riff24Khz16BitMonoPcm format, which is suitable for high-quality audio output.\n",
    "    You can change this to other formats as needed. \"\"\"\n",
    "speech_config.set_speech_synthesis_output_format(speech_sdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)\n",
    "\n",
    "# Set the voice to be used for speech synthesis\n",
    "# This example uses the en-GB-George voice, but you can choose any available voice\n",
    "speech_sdk.SpeechConfig.speech_synthesis_voice_name = \"en-GB-George\"\n",
    "\n",
    "# Initialize the AudioConfig to output to the default speaker\n",
    "audio_config = speech_sdk.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "# Create a SpeechSynthesizer object\n",
    "speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# Text to synthesize\n",
    "text_to_speak = \"Hello, this is a test of the Azure AI Speech service.\"\n",
    "\n",
    "# Start speech synthesis\n",
    "result = speech_synthesizer.speak_text_async(text_to_speak).get()\n",
    "\n",
    "# Check the result\n",
    "if result.reason == speech_sdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesis completed successfully.\")\n",
    "elif result.reason == speech_sdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "    if cancellation_details.reason == speech_sdk.CancellationReason.Error:\n",
    "        print(f\"Error details: {cancellation_details.error_details}\")\n",
    "else:\n",
    "    print(f\"Unexpected result: {result.reason}\")\n",
    "# End of code to run speech synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43d5f4",
   "metadata": {},
   "source": [
    "# Configure audio format and voices\n",
    "\n",
    "When synthesizing speech, you can use a SpeechConfig object to customize the audio that is returned by the Azure AI Speech service.\n",
    "### Audio format\n",
    "The Azure AI Speech service supports multiple output formats for the audio stream that is generated by speech synthesis. Depending on your specific needs, you can choose a format based on the required:\n",
    "\n",
    "- Audio file type\n",
    "- Sample-rate\n",
    "- Bit-depth\n",
    "\n",
    "For example, the following Python code sets the speech output format for a previously defined SpeechConfig object named *speech_config*:\n",
    "\n",
    "```speech_config.set_speech_synthesis_output_format(speech_sdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)```\n",
    "\n",
    "For a full list of supported formats and their enumeration values, see the <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts\">Azure AI Speech SDK documentation</a>.\n",
    "### Voices\n",
    "The Azure AI Speech service provides multiple voices that you can use to personalize your speech-enabled applications. Voices are identified by names that indicate a locale and a person's name - for example en-GB-George.\n",
    "The following Python example code sets the voice to be used\n",
    "\n",
    "```speech_sdk.SpeechConfig.speech_synthesis_voice_name = \"en-GB-George\"```\n",
    "\n",
    "For information about voices, see the Azure AI Speech SDK documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "210c37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config.set_speech_synthesis_output_format(speech_sdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)\n",
    "speech_sdk.SpeechConfig.speech_synthesis_voice_name = \"en-GB-George\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f324171",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "While the Azure AI Speech SDK enables you to submit plain text to be synthesized into speech, the service also supports an XML-based syntax for describing characteristics of the speech you want to generate. This Speech Synthesis Markup Language (SSML) syntax offers greater control over how the spoken output sounds, enabling you to:\n",
    "- Specify a speaking style, such as \"excited\" or \"cheerful\" when using a neural voice.\n",
    "- Insert pauses or silence.\n",
    "- Specify phonemes (phonetic pronunciations), for example to pronounce the text \"SQL\" as \"sequel\".\n",
    "- Adjust the prosody of the voice (affecting the pitch, timbre, and speaking rate).\n",
    "- Use common \"say-as\" rules, for example to specify that a given string should be expressed as a date, time, telephone number, or other form.\n",
    "- Insert recorded speech or audio, for example to include a standard recorded message or simulate background noise.\n",
    "For example, consider the following SSML:\n",
    "```xml\n",
    "<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" \n",
    "                     xmlns:mstts=\"https://www.w3.org/2001/mstts\" xml:lang=\"en-US\"> \n",
    "    <voice name=\"en-US-AriaNeural\"> \n",
    "        <mstts:express-as style=\"cheerful\"> \n",
    "          I say tomato \n",
    "        </mstts:express-as> \n",
    "    </voice> \n",
    "    <voice name=\"en-US-GuyNeural\"> \n",
    "        I say <phoneme alphabet=\"sapi\" ph=\"t ao m ae t ow\"> tomato </phoneme>. \n",
    "        <break strength=\"weak\"/>Lets call the whole thing off! \n",
    "    </voice> \n",
    "</speak>\n",
    "```\n",
    "This SSML specifies a spoken dialog between two different neural voices, like this:\n",
    "- Ariana (cheerfully): \"I say tomato:\n",
    "- Guy: \"I say tomato (pronounced tom-ah-toe) ... Let's call the whole thing off!\"\n",
    "To submit an SSML description to the Speech service, you can use an appropriate method of a SpeechSynthesizer object, like this:\n",
    "\n",
    "``` python\n",
    "speech_synthesizer.speak_ssml('<speak>...');\n",
    "```\n",
    "\n",
    "For more information about SSML, see the <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-synthesis-markup\"> Azure AI Speech SDK documentation. </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673bd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go and run the speaking_clock to see an application of this code ideals!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiengazure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
